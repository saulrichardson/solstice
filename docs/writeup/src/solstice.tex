\documentclass[10pt,twocolumn]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{shapes,arrows,positioning}

% Statistics removed - no longer auto-generated

\title{\textbf{Solstice: LLM-Orchestrated System for Medical Document Fact-Checking}}
\author{An AI-Native Approach to Evidence Extraction and Verification}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Solstice is a multi-step system that automatically fact-checks medical claims against scientific literature and clinical documents. It combines layout detection, multimodal language models, and an orchestrated chain of LLM calls to extract and verify evidence from PDFs that contain text, tables, and figures.

The system is designed to handle real-world medical documentation at scale, processing complex clinical documents containing text, tables, and figures.

\section{System Architecture}

\subsection{Document Ingestion Pipeline}

The ingestion pipeline transforms unstructured PDFs into queryable structured documents through multiple stages:

\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Layout Detection}: Uses Detectron2 with PubLayNet-trained models (Faster R-CNN + ResNet-50-FPN). The pipeline relies on pre-trained weights rather than custom training because PubLayNet provides extensive annotated data.
\item \textbf{Box Consolidation}: Resolves overlapping detections through IoU-based merging (threshold 0.7) and hierarchical nesting. Medical PDFs often contain overlapping layout elements, so the implementation includes domain-specific rules.
\item \textbf{Text Extraction}: Uses PyMuPDF for vector text extraction within bounding boxes, and falls back to Tesseract OCR for scanned pages. SymSpell correction fixes common OCR artefacts (e.g., "rn" → "m").
\item \textbf{Figure/Table Extraction}: Saves visual elements as PNG at 300 DPI. Images are stored separately for later multimodal analysis rather than embedded as base64.
\item \textbf{Reading Order}: Computes reading order with column detection and vertical positioning; this is necessary for the multi-column layouts common in medical journals.
\end{enumerate}

\subsection{LLM-Based Fact-Checking System}

The fact-checking pipeline is composed of specialised LLM calls orchestrated to work together:

\begin{itemize}[leftmargin=*,topsep=0pt]
\item \textbf{Evidence Extraction Step}: Searches document text for claim-relevant quotes using GPT-4 with temperature=0. Preserves exact quotes while correcting OCR errors and returns structured evidence with relevance explanations.

\item \textbf{Evidence Verification Step (V2)}: Validates that extracted quotes exist in the source document. Uses semantic matching to handle OCR variations and filters out tangentially related content, achieving high verification rates.

\item \textbf{Completeness Checker}: Takes the raw extracted evidence and searches for any additional quotes that weren't initially found. Uses the same extraction approach but explicitly excludes already-found quotes to expand coverage.

\item \textbf{Image Evidence Analyzer}: Analyzes figures and tables using vision models to identify supporting visual evidence. Processes images in parallel with semaphore control (max 5 concurrent) and provides detailed explanations.

\item \textbf{Evidence Presenter}: Consolidates all verified text and image evidence into structured JSON and HTML reports. Assesses overall evidence coverage (complete/partial/none) and produces human-readable summaries.
\end{itemize}

\section{Design Decisions}

\subsection{Step Communication Pattern}
Chose filesystem-based communication over message passing:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Each step reads inputs from disk and writes outputs to disk
\item No shared memory or direct step-to-step communication
\item Provides debuggability, resumability, and testability in isolation
\item Introduces disk I/O overhead, which is acceptable at the current throughput requirements
\end{itemize}

\subsection{Prompt Strategy}
Prompt engineering decisions:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Structured Output}: Always request JSON with explicit schemas
\item \textbf{Few-Shot Examples}: Avoided in favor of clear instructions (reduced token usage)
\item \textbf{Error Context}: Include previous failures in retry prompts
\item \textbf{Temperature=0}: Consistency critical for medical facts
\end{itemize}

\subsection{Caching Strategy}
Multi-level caching approach:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Document Cache}: Extracted content never re-processed
\item \textbf{Call Cache}: Each step output stored by claim ID
\item \textbf{No LLM Response Cache}: The pipeline always fetches fresh responses
\item Caching is applied at semantic boundaries rather than individual API calls
\end{itemize}

\section{Technical Implementation}

\subsection{Orchestration Layer}

The system uses asynchronous Python with strategic architectural decisions:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Hierarchical Orchestration}: Two-level design with StudyOrchestrator → ClaimOrchestrator → processing steps. Enables both study-level and claim-level parallelism control.
\item \textbf{Resource-Aware Parallelism}: Default 2 concurrent claims based on empirical testing showing memory usage of ~2GB per claim with GPT-4 context windows.
\item \textbf{Filesystem-Based State}: All intermediate results are written to disk, which makes runs resumable and easier to debug at the cost of additional disk I/O.
\item \textbf{Step Isolation}: Each processing step runs independently with explicit input/output contracts via Pydantic models. Enables testing and development in isolation.
\end{itemize}

\subsection{Where to Find the Data Artifacts}

Solstice writes intermediate and final results to disk in human-readable form. This allows users to inspect the system, debug individual stages, or create visualisations without rerunning the full pipeline.

\paragraph{1. Formatted Claims \\& Evidence}
After you run a fact-checking study (\texttt{python -m src.cli run-study}) the full, merged JSON for each claim is
written to:
\begin{verbatim}
data/studies/<study_name>/
├── study_report.json            # roll-up across all claims
└── claim_<id>/
    └── evidence_report.json     # structured claims + supporting / refuting evidence
\end{verbatim}
The \texttt{evidence\_report.json} file is what drives the Streamlit demo and can be consumed directly by any UX
layer.

If you want the raw, per-step outputs (e.g. to analyse the verifier’s chain-of-thought) look under the same
\texttt{claim\_<id>/agent\_outputs/} folder where each processing step writes an \texttt{output.json} plus helper metadata.

\paragraph{2. Marketing-Material Cache}
PDFs passed through the dedicated marketing pipeline end up in:
\begin{verbatim}
data/marketing_cache/<pdf_name>/
├── extracted/content.json   # text + layout (marketing-tuned)
└── figures/                 # high-resolution figure crops
\end{verbatim}
The structure mirrors \texttt{scientific\_cache} so the same downstream tools can ingest either kind of document.

\paragraph{3. Quick Directory Cheatsheet}
For newcomers, these are useful places to explore:
\begin{itemize}[leftmargin=*]
  \item \texttt{src/cli/} – entry-point scripts that orchestrate the pipelines.
  \item \texttt{src/injestion/shared/processing/} – layout detection, reading-order logic, text extractors.
  \item \texttt{src/fact\_check/agents/} – implementation of the five specialised LLM calls.
  \item \texttt{data/scientific\_cache/} – processed scientific PDFs (inspect \texttt{content.json}).
  \item \texttt{data/marketing\_cache/} – processed marketing PDFs.
  \item \texttt{data/studies/} – end-to-end fact-checking results ready for consumption.
\end{itemize}

\subsection{Model Integration}

Models were selected based on empirical performance:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{GPT-4 (Primary)}: Evidence extraction and verification with temperature=0 to prioritise consistency over creativity.
\item \textbf{Vision Models}: GPT-4V for image analysis. Images are processed individually with focused prompts rather than in batches.
\item \textbf{Gateway Pattern}: All LLM calls go through an HTTP gateway service, which handles rate limiting, cost tracking, and provider abstraction.
\item \textbf{Prompt Engineering}: Uses structured output formats with explicit JSON schemas. Token limits were removed after issues with truncation.
\end{itemize}

\subsection{Robust Error Handling}

The implementation includes several measures to improve reliability:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Defensive JSON Parsing}: Custom parser handles markdown code blocks, trailing commas, and incomplete responses. Learned from: 15\% of GPT-4 responses wrapped JSON in markdown.
\item \textbf{Smart Retry Strategy}: Exponential backoff with error context injection. Including parsing errors in retry prompts increased the success rate to 95\%+.
\item \textbf{Pydantic Validation}: Type-safe data flow between processing steps; the pipeline fails fast with clear errors rather than propagating invalid data.
\item \textbf{Context Window Management}: The max\_tokens parameter was removed after 30\% of evidence was found to be truncated; this increases token cost but keeps responses complete.
\end{itemize}

\section{Implementation Notes}

\subsection{Multimodal Evidence Integration}
Visual evidence is handled in a separate image extraction and analysis pipeline:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Images are stored as files, not base64, which makes browser-based inspection easier
\item Images are analysed individually with focused prompts rather than in batches
\item Parallel processing is limited with a semaphore (max 5 concurrent calls) to avoid API rate limits
\end{itemize}

\subsection{Three-Stage Evidence Pipeline}
Extraction, verification, and completeness are run as distinct LLM-driven steps:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Stage 1}: Extract all potentially relevant quotes (high recall)
\item \textbf{Stage 2}: Verify quotes exist and support claim (high precision)
\item \textbf{Stage 3}: Find additional quotes not caught in initial extraction (expand coverage)
\end{itemize}
This separation allows the stages to be optimised and tested independently.

\subsection{OCR-Resilient Text Matching}
The pipeline includes a fuzzy matching algorithm to address common OCR issues:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Character-level substitution rules ("0"/"O", "1"/"l", "rn"/"m")
\item Whitespace normalization for column-spanning text
\item Semantic similarity fallback using sentence embeddings
\end{itemize}

\subsection{Filesystem-Centric Architecture}
The pipeline stores intermediate data on the filesystem rather than in a database:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Human-readable JSON can be inspected without extra tools
\item Hierarchical directories (document/claim/step) reflect the processing flow
\item No additional infrastructure is required
\item Files can be version-controlled in Git
\end{itemize}


\section{Conclusion}

Solstice combines layout understanding, multimodal analysis, and orchestrated LLM calls to perform medical fact-checking. Tests on multiple documents and claims indicate that the architecture can process real-world medical documentation at moderate scale.

\end{document}
