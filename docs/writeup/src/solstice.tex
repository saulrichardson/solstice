\documentclass[10pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{shapes,arrows,positioning}

% Statistics removed - no longer auto-generated

\title{\textbf{Solstice: LLM-Orchestrated System for Medical Document Fact-Checking}}
\author{An AI-Native Approach to Evidence Extraction and Verification}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Solstice is a multi-step system that automatically fact-checks medical claims against scientific literature and clinical documents. It combines layout detection, multimodal language models, and an orchestrated chain of LLM calls to extract and verify evidence from PDFs that contain text, tables, and figures.

The system is designed to handle real-world medical documentation at scale, processing complex clinical documents containing text, tables, and figures.

\section{System Architecture}

\subsection{Document Ingestion Pipeline}

The ingestion pipeline transforms unstructured PDFs into queryable structured documents through multiple stages:

\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Layout Detection}: Uses Detectron2 with PubLayNet-trained models (Mask R-CNN + ResNet-50-FPN). The pipeline relies on pre-trained weights rather than custom training because PubLayNet provides extensive annotated data.
\item \textbf{Box Consolidation}: Merges overlapping layout detections and resolves conflicts. Medical PDFs often contain complex overlapping elements that require special handling.
\item \textbf{Text Extraction}: Uses PyMuPDF for vector text extraction within bounding boxes.
\item \textbf{Figure/Table Extraction}: Saves visual elements as PNG at 400 DPI. Images are stored separately for later multimodal analysis rather than embedded as base64.
\item \textbf{Reading Order}: Computes reading order with column detection and vertical positioning; this is necessary for the multi-column layouts common in medical journals.
\end{enumerate}

\subsection{LLM-Based Fact-Checking System}

The fact-checking pipeline processes text evidence through three sequential stages (Extract → Completeness → Verify) followed by parallel image analysis:

\begin{itemize}[leftmargin=*,topsep=0pt]
\item \textbf{Evidence Extraction Step}: Searches document text for claim-relevant quotes using gpt-4.1 with temperature=0. Preserves exact quotes and returns structured evidence with relevance explanations.

\item \textbf{Completeness Checker}: Takes the raw extracted evidence and searches for any additional quotes that weren't initially found. Merges evidence from multiple sources to ensure comprehensive coverage.

\item \textbf{Evidence Verification Step (V2)}: Validates that all extracted quotes (from both extraction and completeness steps) exist in the source document. Uses semantic matching and filters out tangentially related content, achieving high verification rates.

\item \textbf{Image Evidence Analyzer}: Analyzes figures and tables using vision models to identify supporting visual evidence. Processes images in parallel with semaphore control (max 5 concurrent) and provides detailed explanations.
\end{itemize}

After all LLM processing completes, an Evidence Presenter step consolidates all verified text and image evidence into structured JSON reports with coverage assessment.

\section{Technical Implementation}

\subsection{Orchestration Layer}

The system uses asynchronous Python with strategic architectural decisions:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Hierarchical Orchestration}: Two-level design with StudyOrchestrator → ClaimOrchestrator → processing steps. Enables both study-level and claim-level parallelism control.
\item \textbf{Step Isolation}: Each processing step runs independently with explicit input/output contracts via Pydantic models. Enables testing and development in isolation.
\end{itemize}

\subsection{Where to Find the Data Artifacts}

Solstice writes intermediate and final results to disk in human-readable form. This allows users to inspect the system, debug individual stages, or create visualisations without rerunning the full pipeline.

\paragraph{1. Fact-Checking Study Results}
After you run a fact-checking study (\texttt{python -m src.cli run-study}), the main results file containing all claims with their supporting text, tables, and figures is located at:

\begin{verbatim}
data/studies/consolidated_results.json
\end{verbatim}

This single JSON file contains all the verified evidence for each claim, including:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item The original claim text
\item Supporting and refuting text quotes from documents
\item References to relevant tables with extracted data
\item References to figures with visual evidence
\item Verification status and confidence scores
\end{itemize}

For developers who need to debug or analyze individual processing steps, the system also maintains a detailed directory structure under \texttt{data/studies/<study\_name>/} with intermediate outputs from each agent, but most users will only need the consolidated results file.

\paragraph{2. Marketing Material Analysis}
Marketing PDFs undergo specialized processing with enhanced visualization extraction. The results are stored in:

\begin{verbatim}
data/marketing_cache/<pdf_name>/
|-- extracted/
|   |-- content.json               # Full document structure and text
|   |-- metadata.json              # Document metadata and processing info
|   `-- reading_order.json         # Computed reading sequence
|-- figures/
|   |-- figure_1.png               # High-resolution figure at 400 DPI
|   |-- figure_2.png               
|   |-- ...
|   `-- figure_metadata.json       # Bounding boxes and captions
|-- tables/
|   |-- table_1.png                # Table images for visual analysis
|   |-- table_2.png
|   |-- ...
|   `-- table_content.json         # Extracted table data
|-- visualizations/                # ** KEY DIRECTORY FOR MARKETING VISUALS **
|   |-- layout_annotated.pdf       # PDF with layout boxes overlaid
|   |-- text_flow_diagram.png      # Reading order visualization
|   `-- element_hierarchy.json     # Structural hierarchy data
`-- cache_info.json                # Processing timestamp and version
\end{verbatim}

The \textbf{visualizations/} directory is particularly important for marketing materials as it contains annotated versions of the document showing detected layout elements, which helps verify proper extraction of complex marketing layouts.

\paragraph{3. Quick Directory Cheatsheet}
For newcomers, these are useful places to explore:
\begin{itemize}[leftmargin=*]
  \item \texttt{src/cli/} – entry-point scripts that orchestrate the pipelines.
  \item \texttt{src/injestion/shared/processing/} – layout detection, reading-order logic, text extractors.
  \item \texttt{src/fact\_check/agents/} – implementation of the four specialised LLM processing steps plus formatting.
  \item \texttt{data/scientific\_cache/} – processed scientific PDFs (inspect \texttt{content.json}).
  \item \texttt{data/marketing\_cache/} – processed marketing PDFs.
  \item \texttt{data/studies/} – end-to-end fact-checking results ready for consumption.
\end{itemize}

\subsection{Model Integration}

Models were selected based on empirical performance:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{gpt-4.1 (Primary)}: Evidence extraction and verification with temperature=0 to prioritise consistency over creativity.
\item \textbf{Vision Models}: o4-mini for image analysis. Images are processed individually with focused prompts rather than in batches.
\item \textbf{Gateway Pattern}: All LLM calls go through an HTTP gateway service, which handles rate limiting, cost tracking, and provider abstraction.
\item \textbf{Prompt Engineering}: Uses structured output formats with explicit JSON schemas. Token limits were removed after issues with truncation.
\end{itemize}


\section{Implementation Notes}

\subsection{Multimodal Evidence Integration}
Visual evidence is handled in a separate image extraction and analysis pipeline:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Images are stored as files, not base64, which makes browser-based inspection easier
\item Images are analysed individually with focused prompts rather than in batches
\item Parallel processing is limited with a semaphore (max 5 concurrent calls) to avoid API rate limits
\end{itemize}

\subsection{Three-Stage Evidence Pipeline}
Extraction, verification, and completeness are run as distinct LLM-driven steps:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item \textbf{Stage 1}: Extract all potentially relevant quotes (high recall)
\item \textbf{Stage 2}: Find additional quotes not caught in initial extraction (expand coverage)
\item \textbf{Stage 3}: Verify all quotes exist and support claim (high precision)
\end{itemize}
This separation allows the stages to be optimised and tested independently.

\subsection{Filesystem-Centric Architecture}
The pipeline stores intermediate data on the filesystem rather than in a database:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item Human-readable JSON can be inspected without extra tools
\item Hierarchical directories (document/claim/step) reflect the processing flow
\item No additional infrastructure is required
\item Files can be version-controlled in Git
\end{itemize}


\section{Conclusion}

Solstice combines layout understanding, multimodal analysis, and orchestrated LLM calls to perform medical fact-checking. Tests on multiple documents and claims indicate that the architecture can process real-world medical documentation at moderate scale.

\end{document}
