<!--
  This file was generated by moving the detailed portions of the original
  repository README into the new docs folder.  It serves as an in-depth
  project overview while the top-level README stays concise.
-->

<!-- NOTE: This is the content previously found in the large root README.  It
     has been moved verbatim so that the root README can remain a concise
     landing page.  Feel free to further break this file into smaller docs/*
     sections as the project evolves. -->


# Solstice

A clinical document processing pipeline with advanced layout detection and fact-checking capabilities.

## Overview

Solstice is a comprehensive system for processing clinical documents (PDFs, clinical trial data, FDA documents) that combines state-of-the-art document analysis with AI-powered fact-checking. The system is designed to handle complex medical and scientific documentation, extracting structured information and verifying claims against evidence.

### Repository Structure

```
solstice/
├── src/                    # Main source code
│   ├── cli/               # Command-line interface for all operations
│   ├── fact_check/        # Multi-agent fact-checking system
│   │   ├── agents/        # Individual evidence extraction agents
│   │   ├── orchestrators/ # Coordination of agent pipelines
│   │   └── utils/         # Fact-checking utilities
│   ├── gateway/           # API proxy service for LLM interactions
│   ├── injestion/         # Document processing pipelines
│   │   ├── scientific/    # Main pipeline for clinical/scientific PDFs
│   │   ├── marketing/     # Specialized pipeline for marketing materials
│   │   └── shared/        # Common processing utilities
│   │       ├── processing/    # Text correction and cleaning
│   │       ├── storage/       # Data persistence layer
│   │       └── visualization/ # Visual output generation
│   ├── core/              # Core utilities and logging
│   ├── interfaces/        # Shared data models and interfaces
│   └── util/              # Helper utilities
├── data/                  # Data directory (see data/README.md)
│   ├── cache/            # Processed document outputs
│   ├── claims/           # Claim definition files
│   ├── clinical_files/   # Input PDFs (clinical/scientific)
│   ├── marketing_slide/  # Input PDFs (marketing)
│   └── studies/          # Fact-checking study results
├── docker-compose.yml     # Container orchestration
├── Makefile              # Build and operation commands
└── pyproject.toml        # Package configuration
```

### Core Components

1. **Document Ingestion Pipeline** (`src/injestion/`)
   - Converts PDFs into structured, searchable JSON
   - Extracts text, tables, figures, and metadata
   - Corrects common PDF text extraction errors
   - Maintains document structure and relationships
   - Handles both scientific papers and marketing materials

2. **Fact-Checking System** (`src/fact_check/`)
   - Verifies claims against extracted document content
   - Multi-agent system where each agent has a specific role:
     - Finding relevant evidence passages
     - Validating that evidence supports claims
     - Identifying missing evidence
     - Analyzing charts and figures
   - Creates detailed evidence trails for transparency
   - Outputs structured verification reports

3. **Gateway Service** (`src/gateway/`)
   - Manages all LLM API interactions
   - Provides centralized request handling
   - Implements caching to reduce API costs
   - Handles errors and retries gracefully
   - Tracks usage and performance metrics

### Key Features

- **Accurate PDF Processing**: Extracts text, tables, and figures from complex medical documents
- **Intelligent Text Correction**: Automatically fixes PDF extraction errors while preserving medical terminology
- **Evidence-Based Verification**: Fact-checks claims by finding and validating supporting evidence in documents
- **Transparent Results**: Every claim verification includes exact quotes and page references
- **Production Ready**: Scalable deployment with monitoring and error handling

### Use Cases

- Processing clinical trial protocols and results
- Extracting data from FDA submissions and approvals
- Verifying claims in medical literature
- Building structured datasets from unstructured clinical documents
- Analyzing marketing materials for regulatory compliance

### How It Works

1. **Place PDFs** in `data/clinical_files/` or `data/marketing_slide/`
2. **Run ingestion** to extract structured content: `python -m src.cli ingest`
3. **Run fact-checking** to verify claims: `python -m src.cli run-study`
4. **View results** in `data/cache/` (extracted content) and `data/studies/` (fact-check results)

The system processes documents through a pipeline:
- PDF → Layout Detection → Text Extraction → Structured JSON
- Claims + Documents → Evidence Extraction → Verification → Study Results

## Architecture Details

### Document Processing Flow

```
Input PDFs → Ingestion Pipeline → Structured JSON → Fact-Checking → Evidence Results
                     ↓                                      ↓
              Layout Detection                       Multi-Agent System
              Text Correction                        Evidence Extraction
              Table/Figure Extraction                Verification
                                                    Image Analysis
```

### Module Responsibilities

**CLI Module** (`src/cli/`): Command-line interface
- `ingest`: Convert PDFs to structured JSON
- `run-study`: Run fact-checking on claims
- `gateway`: Start/stop API service

**Ingestion Module** (`src/injestion/`): Document processing
- **Scientific Pipeline**: For clinical trials, FDA documents, research papers
  - Handles dense text, complex tables, scientific figures
  - Preserves citations and cross-references
- **Marketing Pipeline**: For presentations and marketing materials  
  - Better handling of visual layouts and branding
  - Extracts marketing claims and visual evidence
- **Text Processing**: Corrects extraction errors, preserves medical terms

**Fact-Check Module** (`src/fact_check/`): Claim verification
- **Evidence Pipeline**:
  1. Extract relevant passages from documents
  2. Verify passages actually support claims
  3. Check for missing evidence
  4. Analyze supporting images/charts
  5. Generate final evidence report
- **Orchestration**: Manages parallel processing of multiple claims

**Gateway Module** (`src/gateway/`): API management
- Proxies requests to OpenAI/other LLMs
- Implements rate limiting and retries
- Caches responses to reduce costs
- Provides usage analytics

### Data Flow

1. **Input**: PDFs placed in `data/clinical_files/` or `data/marketing_slide/`
2. **Processing**: Ingestion extracts content to `data/cache/{document_name}/`
3. **Fact-Checking**: Agents process claims and save to `data/cache/{document_name}/agents/`
4. **Results**: Final study results saved to `data/studies/`

### Text Extraction and Correction

The system employs a multi-stage approach to extract and correct text from PDFs:

1. **Raw Text Extraction**
   - Uses PyMuPDF (fitz) to extract text while preserving document structure
   - Maintains paragraph boundaries and text flow
   - Extracts tables as structured data when possible

2. **Intelligent Text Correction**
   - **Spacing Correction**: Fixes concatenated words common in PDFs
     - "theinformationneeded" → "the information needed"
     - Uses WordNinja with Google n-gram frequencies
   - **Medical Term Preservation**: Maintains specialized terminology
     - Preserves drug names, medical procedures, dosages
     - Protects registered trademarks (e.g., "Flublok®")
   - **Context-Aware Processing**: Different rules for different content
     - Scientific text: preserves chemical formulas, units
     - Marketing text: maintains brand styling

3. **LLM-Assisted Enhancement** (Fact-Checking Phase)
   - LLMs analyze extracted text for evidence relevance
   - **Guardrails prevent hallucination**:
     - LLMs can only reference text explicitly present in documents
     - All evidence must include exact quotes with page numbers
     - Verification agents double-check all extracted evidence
   - **No content generation**: LLMs extract and verify, never create

This multi-layered approach ensures high-quality text extraction while preventing AI-generated content from contaminating the evidence base.

## Installation

This guide will walk you through setting up Solstice step by step. The setup has two parts:
1. **Basic setup** (required) - Core functionality for text extraction and fact-checking
2. **Advanced setup** (optional) - Adds AI-powered layout detection for complex PDFs

### Step 1: Check Prerequisites

- **Python 3.11 or 3.12** (required for Detectron2 compatibility)
  - Python 3.13+ is not yet supported by Detectron2
  - We recommend using [pyenv](https://github.com/pyenv/pyenv) or [conda](https://docs.conda.io/) to manage Python versions
  
  **Automatic version management with pyenv:**
  ```bash
  # Install pyenv (if not already installed)
  # macOS: brew install pyenv
  # Linux: curl https://pyenv.run | bash
  
  # Install Python 3.11.9 (exact version used by this project)
  pyenv install 3.11.9
  
  # The project includes a .python-version file that will automatically
  # activate Python 3.11.9 when you enter the directory
  cd solstice
  python --version  # Should show Python 3.11.9
  ```
- **Poppler** (for PDF processing)
  - macOS: `brew install poppler`
  - Ubuntu/Debian: `sudo apt-get install poppler-utils`
  - Windows: Download from [poppler releases](https://github.com/oschwartz10612/poppler-windows/releases)

### Step 2: Installation - Complete Example

Here's the exact workflow to get everything installed from scratch:

#### 2.1 Clone the repository

```bash
git clone <repository-url>
cd solstice
```

#### 2.2 Set up Python 3.11 (CRITICAL - detectron2 requires 3.11 or 3.12)

**Option A: If you have pyenv installed:**
```bash
# The project includes a .python-version file that specifies 3.11.9
# Pyenv will automatically use it when you enter the directory
cd solstice
python --version  # Should show Python 3.11.x
```

**Option B: If you DON'T have pyenv:**
```bash
# Check your Python version
python3 --version

# If it's not 3.11.x or 3.12.x, you need to install Python 3.11:
# macOS: brew install python@3.11
# Ubuntu: sudo apt install python3.11 python3.11-venv
# Then use python3.11 explicitly in the next steps
```

#### 2.3 Create and activate virtual environment

```bash
# IMPORTANT: If your default python3 is NOT 3.11, use python3.11 explicitly:
python3.11 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate  # On macOS/Linux

# Verify you're using the right Python version
python --version  # Must show Python 3.11.x or 3.12.x
```

**Common issue:** If you see Python 3.13.x, you created the venv with the wrong Python version. Delete it and recreate:
```bash
deactivate
rm -rf .venv
python3.11 -m venv .venv
source .venv/bin/activate
```

#### 2.4 Install the base package

```bash
# Upgrade pip first (important!)
pip install --upgrade pip

# Install the base package
make install
```

This will:
- Verify Python version is 3.11 or 3.12
- Install all core dependencies
- Set up the package in development mode

#### 2.5 (Optional but recommended) Install detectron2 for layout detection

For processing complex PDFs with tables and figures:

```bash
make install-detectron2
```

**What this installs:**
- PyTorch and torchvision for deep learning
- Detectron2 for state-of-the-art object detection (built from source)
- LayoutParser for document structure analysis
- **Patched iopath** to fix model download issues (installed after detectron2)

**Note about the installation:**
- Detectron2 is built from source since pre-built wheels aren't available for Python 3.11+
- The installation uses `--no-build-isolation` to ensure torch is available during build
- A patched version of iopath is installed to fix the `?dl=1` query parameter issue
- The iopath cache is cleared before installation to avoid conflicts

**Notes:**
- This takes 5-10 minutes (builds from source)
- Requires ~2GB of disk space
- Downloads AI models on first use
- Skip this if you only need basic text extraction


#### 2.6 Verify everything works

```bash
make verify
```

Expected output:
```
✓ fact_check package installed
✓ gateway package installed  
✓ OpenAI library installed
✓ LayoutParser installed
✓ Poppler installed

Python: Python 3.11.13
Pip:    pip 25.1.1 from /path/to/.venv/lib/python3.11/site-packages/pip (python 3.11)
```

<details>
<summary><strong>Complete Installation Example (click to expand)</strong></summary>

```bash
# 1. Clone and enter the project
git clone <repository-url>
cd solstice

# 2. Check Python version
python3 --version
# If not 3.11.x, install it (e.g. on macOS: brew install python@3.11)

# 3. Create virtual environment with Python 3.11
python3.11 -m venv .venv
source .venv/bin/activate

# 4. Verify the version inside the venv
python --version  # Must show Python 3.11.x

# 5. Upgrade build tools (mitigates most build errors)
pip install --upgrade pip wheel setuptools

# 6. Install core dependencies
make install

# 7. (Optional) Install Detectron2 for layout detection
make install-detectron2

# 8. Verify everything is in place
make verify

# 9. Provide your OpenAI key
cp .env.example .env && echo "OPENAI_API_KEY=sk-…" >> .env

# 10. Test the CLI
python -m src.cli --help
```

</details>

### Step 3: Configuration

#### 3.1 Set up environment file

Copy the example environment file:

```bash
cp .env.example .env
```

Edit `.env` and add your OpenAI API key:
```
OPENAI_API_KEY=sk-your-api-key-here
```

Note: The gateway service requires this for API calls. Leave other settings at defaults.

#### 4.2 Data Directories

The project expects this structure:
```
data/
├── clinical_files/   # Put your input PDFs here
└── cache/           # Processed outputs go here
```

Create them if needed:
```bash
mkdir -p data/clinical_files data/cache
```

### Step 5: Quick Start

Now you're ready to use Solstice! Here are the main workflows:

#### Process PDFs

Place your PDFs in `data/clinical_files/`, then:

```bash
python -m src.cli ingest
```

This will:
- Process all PDFs in the input directory
- Extract text with intelligent spacing correction
- Detect and extract tables/figures (if Detectron2 installed)
- Save structured JSON to `data/cache/`

#### Run Fact Checking

After ingesting documents, extract evidence for claims:

```bash
# Run fact-checking pipeline
python -m src.cli run-study
```

This runs the complete fact-checking pipeline that:
- Searches for evidence supporting each claim
- Verifies and validates found evidence
- Checks completeness of evidence
- Analyzes images for supporting evidence
- Presents results in structured format

By default, it uses claims from `data/claims/Flublok_Claims.json` and searches all processed documents.

**Note**: The marketing folder contains a specialized parser specifically designed for Flublok marketing PDFs, using PrimaLayout for better detection of marketing material layouts.

#### Using the Gateway Service

Start the API gateway for production use:

```bash
# Start services (auto-handles Docker issues)
make up

# Check logs
make logs

# Stop when done
make down
```

The gateway runs at `http://localhost:8000` and provides:
- OpenAI-compatible API endpoints
- Request/response logging
- Automatic retries and error handling

### Step 6: Docker Setup (Production)

For production deployment using Docker:

```bash
# Check your Docker setup
make docker-status

# Start all services
docker compose up -d

# Scale gateway instances
docker compose up -d --scale gateway=3
```

**Colima users**: The Makefile automatically restarts Colima if Docker is unresponsive.


## Troubleshooting

### Common Issues

#### Python Version Issues (Most Common Problem)

**Problem:** `make install` fails with "Python 3.11 or 3.12 required (found 3.13.x)"

**Solution:** Your virtual environment was created with the wrong Python version.

```bash
# 1. Deactivate and remove the incorrect venv
deactivate
rm -rf .venv

# 2. Install Python 3.11 if needed
# macOS: brew install python@3.11
# Ubuntu: sudo apt install python3.11 python3.11-venv

# 3. Create venv with correct Python
python3.11 -m venv .venv
source .venv/bin/activate

# 4. Verify
python --version  # Must show 3.11.x or 3.12.x

# 5. Continue installation
pip install --upgrade pip
make install
```

**Problem:** "command not found: python" when creating venv

**Solution:** Use `python3` or `python3.11` explicitly:
```bash
python3.11 -m venv .venv
```

#### Version Conflict Warning

You may see this warning:
```
detectron2 0.6 requires iopath<0.1.10,>=0.1.7, but you have iopath 0.1.11
```

**This is expected and safe to ignore.** We intentionally use a patched iopath that fixes model download issues while remaining API-compatible.

### Layout Detection Issues

If you encounter errors like "Config file does not exist" or issues with model downloads:

1. Clear the iopath cache:
   ```bash
   rm -rf ~/.torch/iopath_cache/
   ```

2. Reinstall detectron2:
   ```bash
   make install-detectron2
   ```



### Memory Issues

Layout detection is memory-intensive. If processing fails on large PDFs:
- Process PDFs individually rather than in batch
- Reduce DPI in the ingestion settings
- Ensure at least 8GB RAM is available

## Development

### Code Quality

```bash
# Format code automatically
make format

# Check code style
make lint

# Run tests
pytest
```

### Project Structure

```
solstice/
├── data/
│   ├── clinical_files/   # Input PDFs go here
│   └── cache/           # Processed outputs stored here
├── src/
│   ├── cli/             # Command-line interface
│   ├── fact_check/      # Fact-checking agents
│   ├── gateway/         # API proxy service  
│   ├── injestion/       # PDF processing pipelines
│   │   ├── scientific/  # Main pipeline for scientific/clinical PDFs
│   │   ├── marketing/   # Specialized pipeline for Flublok marketing PDFs
│   │   └── shared/      # Common utilities
│   │       ├── processing/  # Text cleaning & correction
│   │       ├── storage/     # Data persistence
│   │       └── visualization/ # Visual output generation
│   ├── core/            # Core utilities
│   ├── interfaces/      # Shared interfaces
│   └── util/            # Helper utilities
├── docker-compose.yml   # Container orchestration
├── Makefile            # Common commands
├── pyproject.toml      # Package & dependencies
├── requirements-*.txt  # Dependency constraints
└── .python-version     # Python 3.11.9 (for pyenv)
```

### Adding New Features

1. **New Document Types**: Add parsers in `src/injestion/`
2. **New Fact-Check Agents**: Add to `src/fact_check/agents/`
3. **New API Endpoints**: Modify `src/gateway/`

### Common Commands

```bash
make help         # Show all available commands
make install      # Install base package
make verify       # Check installation
make up          # Start services
make logs        # View logs
make down        # Stop services
make format      # Format code
make lint        # Check code style
make clean       # Remove cache files
```
