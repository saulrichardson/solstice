# Solstice â€“ Clinical Document Fact-Checking Pipeline

Solstice is an **end-to-end research prototype** that takes a pile of PDF clinical documents (drug labels, journal articles, slide decks â€¦) and a list of free-text claims, and returns a structured, evidence-backed verdict for every claim.

ğŸ“„ **[Technical Writeup (PDF)](docs/writeup/solstice.pdf)** - Detailed system architecture, LLM pipeline, and implementation notes

---

## 1. How Solstice fits together

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Solstice System                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Ingestion  â”‚    â”‚   Gateway    â”‚    â”‚ Fact-Check   â”‚    â”‚
â”‚  â”‚   Pipeline   â”‚    â”‚   Service    â”‚    â”‚   Pipeline   â”‚    â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚
â”‚  â”‚ â€¢ PDF â†’ Doc  â”‚    â”‚ â€¢ LLM Proxy  â”‚    â”‚ â€¢ Extract    â”‚    â”‚
â”‚  â”‚ â€¢ Layout Det â”‚    â”‚ â€¢ Audit Log  â”‚    â”‚ â€¢ Verify     â”‚    â”‚
â”‚  â”‚ â€¢ Text Ext   â”‚    â”‚ â€¢ Retry      â”‚    â”‚ â€¢ Present    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                    â”‚                    â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                              â”‚                                  â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                     â”‚  Data Storage   â”‚                        â”‚
â”‚                     â”‚                 â”‚                        â”‚
â”‚                     â”‚ â€¢ Cache         â”‚                        â”‚
â”‚                     â”‚ â€¢ Documents     â”‚                        â”‚
â”‚                     â”‚ â€¢ Evidence      â”‚                        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Processing flow

```
1. Input PDFs (data/clinical_files/)
   - FlublokPI.pdf
   - CDC Influenza vaccines.pdf  
   - Arunachalam et al. (2021).pdf
       â†“
2. Ingestion Pipeline
   - Detectron2 layout detection
   - PyMuPDF text extraction
   - Saves to data/scientific_cache/<document_name>/
       â†“
3. Claims File (data/claims/Flublok_Claims.json)
   - "Flublok is FDA approved for adults 18 years and older"
   - "Flublok demonstrated 30% better efficacy vs standard dose"
       â†“
4. Fact-Check Pipeline (5 LLM calls per claim)
   - Evidence extraction
   - Quote verification  
   - Completeness check
   - Image analysis
   - Evidence presentation
       â†“
5. Output (data/studies/Flu_Vaccine_Claims_Verification/)
   - study_report.json
   - claim_001/evidence_report.json
   - claim_002/evidence_report.json
```

## 3. What happens under the hood?

Below is the *real* (slightly simplified) execution plan so you can map the commands you run to the modules that fire.

--------------------------------------------------------
Step 1: Ingest PDFs â†’ machine-readable artefacts
--------------------------------------------------------

Command: `python -m src.cli ingest`

1. Loader (`src.injestion.shared.loader`)
   â€¢ Scans `data/clinical_files/` for PDF, TIFF or PNG files.  
   â€¢ Streams each file page-by-page to keep memory footprint low.

2. Layout Detection (`src.injestion.shared.processing.layout_detector`)
   â€¢ Uses Detectron2 fine-tuned on 9k annotated FDA labels.  
   â€¢ Returns bounding boxes for *title*, *paragraph*, *table*, *figure* and *footer*.

3. Text Extraction (`src.injestion.shared.processing.text_extractors.pymupdf_extractor`)
   â€¢ Calls PyMuPDF for vector text; falls back to Tesseract OCR if the page is scanned.  
   â€¢ Keeps exact coordinates â†’ later we can highlight snippets in the PDF.

4. Normalisation & Indexing (`src.injestion.shared.processing.reading_order`)
   â€¢ Splits text into ~250-token chunks with a sliding window.  
   â€¢ Saves structured content for semantic search during fact-checking.

Output: Structured JSON per document under `data/scientific_cache/`.

--------------------------------------------------------
Step 2: Run the fact-checking pipeline
--------------------------------------------------------

Command: `python -m src.cli run-study --claims path/to/file.json`

Input objects
â€¢ Claim list  â†’ e.g. "Flublok is FDA approved for adults 18+".  
â€¢ Document set â†’ every processed document in `data/scientific_cache/*`.

Pipeline orchestrator (`src.fact_check.orchestrators.claim_orchestrator`) executes these agents per claim:

1. Evidence Extractor (`agents/evidence_extractor.py`)
   â€¢ Semantic search over all documents to retrieve top-k text chunks (+ figures captions).  
   â€¢ Uses LLM to filter for relevance & returns a ranked list.

2. Completeness Checker (`agents/completeness_checker.py`)
   â€¢ Ensures no critical evidence type was missed (RCT vs observational, safety vs efficacy).  
   â€¢ Merges evidence from multiple sources to ensure comprehensive coverage.

3. Evidence Verifier (`agents/evidence_verifier_v2.py`)
   â€¢ Verifies that extracted quotes exist in the document.  
   â€¢ Confirms quotes genuinely support the claim using chain-of-thought reasoning.

4. Image Evidence Analyzer (`agents/image_evidence_analyzer.py`)
   â€¢ Analyzes all images in the document after text pipeline completes.  
   â€¢ Uses Vision model to determine if images support the claim.

5. Evidence Presenter (`agents/evidence_presenter.py`)
   â€¢ Combines all verified text and image evidence.  
   â€¢ Converts into compact JSON schema + markdown for presentation.

All intermediate LLM calls are cached in `data/studies/<study>/claim_x/agent_outputs/` so re-runs are cheap.

--------------------------------------------------------
Step 3: Gateway & safeguards (optional but recommended)
--------------------------------------------------------

If you started the Docker gateway (`make up`):
â€¢ Rate limiting: honours your OpenAI quota and retries with exponential back-off.  
â€¢ Request proxying: Routes LLM requests through a central service for monitoring.

--------------------------------------------------------
Step 4: Output folder anatomy
--------------------------------------------------------

```
data/studies/Flu_Vaccine_Study/
â”œâ”€â”€ study_report.json              # High-level verdict summary
â”œâ”€â”€ claim_001/
â”‚   â”œâ”€â”€ evidence_report.json       # Merged & cleaned evidence
â”‚   â””â”€â”€ agent_outputs/             # 5 subfolders (one per agent)
â””â”€â”€ ...
```

## 4. Project structure

```
solstice/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/                 # Command-line interface
â”‚   â”œâ”€â”€ injestion/           # PDF processing
â”‚   â”‚   â”œâ”€â”€ scientific/      # Academic paper pipeline  
â”‚   â”‚   â”œâ”€â”€ marketing/       # Marketing material pipeline
â”‚   â”‚   â””â”€â”€ shared/          # Common components
â”‚   â”œâ”€â”€ gateway/             # LLM proxy service
â”‚   â”‚   â””â”€â”€ app/             # FastAPI application
â”‚   â”œâ”€â”€ fact_check/          # Fact-checking pipeline
â”‚   â”‚   â”œâ”€â”€ agents/          # Individual LLM agents
â”‚   â”‚   â”œâ”€â”€ orchestrators/   # Pipeline coordination
â”‚   â”‚   â””â”€â”€ config/          # Model configuration
â”‚   â””â”€â”€ interfaces/          # Shared data models
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clinical_files/      # Input PDFs
â”‚   â”œâ”€â”€ scientific_cache/    # Processed documents
â”‚   â”œâ”€â”€ claims/              # Claim files
â”‚   â””â”€â”€ studies/             # Results
â”œâ”€â”€ docker/                  # Docker configurations
 â””â”€â”€ scripts/                 # Setup utilities
 ```

---

## 5. Quick-start

Prerequisites: Python 3.11â€“3.12, Docker (optional), an OpenAI API key, Poppler (`brew install poppler` / `apt-get install poppler-utils`).

```bash
# 1ï¸âƒ£  Clone the repository
git clone <repo-url> && cd solstice

# 2ï¸âƒ£  Create a virtual env (recommended)
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\\Scripts\\activate

# 3ï¸âƒ£  Install dependencies (plus Detectron2 for layout detection)
make install && make install-detectron2

# 4ï¸âƒ£  Configure OpenAI
cp .env.example .env && echo "OPENAI_API_KEY=sk-..." >> .env

# 5ï¸âƒ£  (Optional) start gateway service for rate-limiting & audit logs
make up   # docker-compose up -d
```

Done! You can now ingest documents and run a fact-checking study:

```bash
# Convert PDFs â†’ structured JSON
python -m src.cli ingest

# Check all flu-vaccine claims
python -m src.cli run-study
```

---

## 6. Installation details

The quick-start should work on most systems. If it doesn't, follow the longer, OS-specific instructions below.

```bash
# 1. Clone & enter repo
git clone <repo-url> && cd solstice

# 2. Create a virtual environment (recommended)
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 3. Core dependencies
make install                # â†³ installs OpenAI, FAISS, PyMuPDF, etc.

# 4. Detectron2 (layout detection)
make install-detectron2     # uses CUDA if available, add CPU_ONLY=1 to force CPU

# 5. Environment variables
cp .env.example .env && echo "OPENAI_API_KEY=sk-..." >> .env

# 6. Optional: start gateway service
make up                     # docker-compose up -d
```

Common pitfalls:
â€¢ macOS M-series + Detectron2 â€“ use `make install-detectron2 CPU_ONLY=1`.  
â€¢ Poppler missing â€“ `brew install poppler` (macOS) / `apt-get install poppler-utils` (Debian/Ubuntu).  
â€¢ OpenAI rate limits â€“ make sure the gateway is up; it automatically retries.

If problems persist, open an issue with the full error message and stack trace.

---

## 7. Run commands

```bash
# Process PDFs into machine-readable documents
python -m src.cli ingest

# Run the marketing document pipeline  
python -m src.cli ingest-marketing

# Fact-check claims against all cached documents
python -m src.cli run-study
```
